#!/usr/bin/env python3
"""
GenAI OS - MediaPipe Consciousness Integration
Real-time multi-modal consciousness processing using MediaPipe
"""

import cv2
import mediapipe as mp
import numpy as np
import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import json

class ConsciousnessMode(Enum):
    """Consciousness processing modes"""
    FACE_ANALYSIS = "face_analysis"
    HAND_TRACKING = "hand_tracking"
    POSE_ESTIMATION = "pose_estimation"
    HOLISTIC = "holistic"
    OBJECT_DETECTION = "object_detection"

@dataclass
class ConsciousnessFrame:
    """Single frame of consciousness data"""
    timestamp: float
    frame_id: int
    mode: ConsciousnessMode
    raw_data: Dict[str, Any]
    processed_data: Dict[str, Any]
    consciousness_score: float
    confidence: float

class MediaPipeConsciousnessProcessor:
    """MediaPipe-based consciousness processing system"""
    
    def __init__(self):
        self.logger = self._setup_logging()
        
        # MediaPipe solutions
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_hands = mp.solutions.hands
        self.mp_pose = mp.solutions.pose
        self.mp_holistic = mp.solutions.holistic
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # Initialize processors
        self.face_detection = None
        self.face_mesh = None
        self.hands = None
        self.pose = None
        self.holistic = None
        
        # Processing state
        self.processing_active = False
        self.current_mode = ConsciousnessMode.HOLISTIC
        self.frame_count = 0
        self.fps_counter = 0
        self.last_fps_time = time.time()
        
        # Consciousness data
        self.consciousness_history: List[ConsciousnessFrame] = []
        self.max_history_size = 1000
        
        # Performance metrics
        self.processing_times = []
        self.consciousness_scores = []
        
        self.logger.info("MediaPipe Consciousness Processor initialized")
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging for MediaPipe consciousness"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(__name__)
    
    async def initialize_processors(self, mode: ConsciousnessMode = None) -> bool:
        """Initialize MediaPipe processors"""
        try:
            if mode is None:
                mode = self.current_mode
            
            if mode == ConsciousnessMode.FACE_ANALYSIS:
                self.face_detection = self.mp_face_detection.FaceDetection(
                    model_selection=1, min_detection_confidence=0.5
                )
                self.face_mesh = self.mp_face_mesh.FaceMesh(
                    max_num_faces=1,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
            
            elif mode == ConsciousnessMode.HAND_TRACKING:
                self.hands = self.mp_hands.Hands(
                    static_image_mode=False,
                    max_num_hands=2,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
            
            elif mode == ConsciousnessMode.POSE_ESTIMATION:
                self.pose = self.mp_pose.Pose(
                    static_image_mode=False,
                    model_complexity=1,
                    smooth_landmarks=True,
                    enable_segmentation=True,
                    smooth_segmentation=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
            
            elif mode == ConsciousnessMode.HOLISTIC:
                self.holistic = self.mp_holistic.Holistic(
                    static_image_mode=False,
                    model_complexity=1,
                    smooth_landmarks=True,
                    enable_segmentation=True,
                    smooth_segmentation=True,
                    refine_face_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
            
            self.current_mode = mode
            self.logger.info(f"Initialized MediaPipe processors for {mode.value}")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize MediaPipe processors: {e}")
            return False
    
    async def process_frame(self, frame: np.ndarray) -> ConsciousnessFrame:
        """Process single frame for consciousness analysis"""
        start_time = time.time()
        
        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Process based on current mode
        raw_data = {}
        processed_data = {}
        consciousness_score = 0.0
        confidence = 0.0
        
        try:
            if self.current_mode == ConsciousnessMode.FACE_ANALYSIS:
                raw_data, processed_data, consciousness_score, confidence = \
                    await self._process_face_analysis(rgb_frame)
            
            elif self.current_mode == ConsciousnessMode.HAND_TRACKING:
                raw_data, processed_data, consciousness_score, confidence = \
                    await self._process_hand_tracking(rgb_frame)
            
            elif self.current_mode == ConsciousnessMode.POSE_ESTIMATION:
                raw_data, processed_data, consciousness_score, confidence = \
                    await self._process_pose_estimation(rgb_frame)
            
            elif self.current_mode == ConsciousnessMode.HOLISTIC:
                raw_data, processed_data, consciousness_score, confidence = \
                    await self._process_holistic(rgb_frame)
            
        except Exception as e:
            self.logger.error(f"Error processing frame: {e}")
            raw_data = {'error': str(e)}
            processed_data = {'error': True}
            consciousness_score = 0.0
            confidence = 0.0
        
        # Create consciousness frame
        consciousness_frame = ConsciousnessFrame(
            timestamp=time.time(),
            frame_id=self.frame_count,
            mode=self.current_mode,
            raw_data=raw_data,
            processed_data=processed_data,
            consciousness_score=consciousness_score,
            confidence=confidence
        )
        
        # Update metrics
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)
        self.consciousness_scores.append(consciousness_score)
        
        # Maintain rolling history
        if len(self.processing_times) > 100:
            self.processing_times.pop(0)
        if len(self.consciousness_scores) > 100:
            self.consciousness_scores.pop(0)
        
        # Add to history
        self.consciousness_history.append(consciousness_frame)
        if len(self.consciousness_history) > self.max_history_size:
            self.consciousness_history.pop(0)
        
        self.frame_count += 1
        
        # Update FPS
        current_time = time.time()
        if current_time - self.last_fps_time >= 1.0:
            self.fps_counter = self.frame_count
            self.frame_count = 0
            self.last_fps_time = current_time
        
        return consciousness_frame
    
    async def _process_face_analysis(self, rgb_frame: np.ndarray) -> Tuple[Dict, Dict, float, float]:
        """Process face analysis for consciousness"""
        raw_data = {}
        processed_data = {}
        consciousness_score = 0.0
        confidence = 0.0
        
        # Face detection
        if self.face_detection:
            face_results = self.face_detection.process(rgb_frame)
            if face_results.detections:
                raw_data['face_detections'] = len(face_results.detections)
                confidence += 0.3
                consciousness_score += 0.2
        
        # Face mesh
        if self.face_mesh:
            mesh_results = self.face_mesh.process(rgb_frame)
            if mesh_results.multi_face_landmarks:
                raw_data['face_landmarks'] = len(mesh_results.multi_face_landmarks[0].landmark)
                
                # Analyze facial expressions
                landmarks = mesh_results.multi_face_landmarks[0].landmark
                processed_data['facial_analysis'] = await self._analyze_facial_expression(landmarks)
                consciousness_score += processed_data['facial_analysis']['expression_intensity']
                confidence += 0.4
        
        return raw_data, processed_data, consciousness_score, confidence
    
    async def _process_hand_tracking(self, rgb_frame: np.ndarray) -> Tuple[Dict, Dict, float, float]:
        """Process hand tracking for consciousness"""
        raw_data = {}
        processed_data = {}
        consciousness_score = 0.0
        confidence = 0.0
        
        if self.hands:
            hand_results = self.hands.process(rgb_frame)
            if hand_results.multi_hand_landmarks:
                raw_data['num_hands'] = len(hand_results.multi_hand_landmarks)
                
                # Analyze hand gestures
                processed_data['hand_analysis'] = []
                for hand_landmarks in hand_results.multi_hand_landmarks:
                    hand_data = await self._analyze_hand_gesture(hand_landmarks.landmark)
                    processed_data['hand_analysis'].append(hand_data)
                    consciousness_score += hand_data['gesture_confidence']
                
                confidence = min(1.0, raw_data['num_hands'] * 0.5)
        
        return raw_data, processed_data, consciousness_score, confidence
    
    async def _process_pose_estimation(self, rgb_frame: np.ndarray) -> Tuple[Dict, Dict, float, float]:
        """Process pose estimation for consciousness"""
        raw_data = {}
        processed_data = {}
        consciousness_score = 0.0
        confidence = 0.0
        
        if self.pose:
            pose_results = self.pose.process(rgb_frame)
            if pose_results.pose_landmarks:
                raw_data['pose_landmarks'] = len(pose_results.pose_landmarks.landmark)
                
                # Analyze body posture
                processed_data['posture_analysis'] = await self._analyze_body_posture(
                    pose_results.pose_landmarks.landmark
                )
                consciousness_score = processed_data['posture_analysis']['alertness_score']
                confidence = processed_data['posture_analysis']['confidence']
        
        return raw_data, processed_data, consciousness_score, confidence
    
    async def _process_holistic(self, rgb_frame: np.ndarray) -> Tuple[Dict, Dict, float, float]:
        """Process holistic analysis for consciousness"""
        raw_data = {}
        processed_data = {}
        consciousness_score = 0.0
        confidence = 0.0
        
        if self.holistic:
            holistic_results = self.holistic.process(rgb_frame)
            
            # Face analysis
            if holistic_results.face_landmarks:
                raw_data['face_landmarks'] = len(holistic_results.face_landmarks.landmark)
                processed_data['face_analysis'] = await self._analyze_facial_expression(
                    holistic_results.face_landmarks.landmark
                )
                consciousness_score += processed_data['face_analysis']['expression_intensity'] * 0.4
                confidence += 0.3
            
            # Hand analysis
            if holistic_results.left_hand_landmarks or holistic_results.right_hand_landmarks:
                hand_count = 0
                processed_data['hand_analysis'] = []
                
                if holistic_results.left_hand_landmarks:
                    hand_count += 1
                    hand_data = await self._analyze_hand_gesture(
                        holistic_results.left_hand_landmarks.landmark
                    )
                    hand_data['hand'] = 'left'
                    processed_data['hand_analysis'].append(hand_data)
                    consciousness_score += hand_data['gesture_confidence'] * 0.2
                
                if holistic_results.right_hand_landmarks:
                    hand_count += 1
                    hand_data = await self._analyze_hand_gesture(
                        holistic_results.right_hand_landmarks.landmark
                    )
                    hand_data['hand'] = 'right'
                    processed_data['hand_analysis'].append(hand_data)
                    consciousness_score += hand_data['gesture_confidence'] * 0.2
                
                raw_data['num_hands'] = hand_count
                confidence += hand_count * 0.2
            
            # Pose analysis
            if holistic_results.pose_landmarks:
                raw_data['pose_landmarks'] = len(holistic_results.pose_landmarks.landmark)
                processed_data['posture_analysis'] = await self._analyze_body_posture(
                    holistic_results.pose_landmarks.landmark
                )
                consciousness_score += processed_data['posture_analysis']['alertness_score'] * 0.4
                confidence += 0.3
        
        # Normalize scores
        consciousness_score = min(1.0, consciousness_score)
        confidence = min(1.0, confidence)
        
        return raw_data, processed_data, consciousness_score, confidence
    
    async def _analyze_facial_expression(self, landmarks) -> Dict[str, Any]:
        """Analyze facial expression for consciousness indicators"""
        # Calculate basic facial metrics
        nose_tip = landmarks[1]  # Nose tip landmark
        left_eye = landmarks[33]  # Left eye landmark
        right_eye = landmarks[263]  # Right eye landmark
        mouth_left = landmarks[61]  # Mouth left
        mouth_right = landmarks[291]  # Mouth right
        
        # Calculate eye openness (simplified)
        eye_distance = abs(left_eye.y - right_eye.y)
        
        # Calculate mouth width
        mouth_width = abs(mouth_left.x - mouth_right.x)
        
        # Simple expression intensity calculation
        expression_intensity = min(1.0, (eye_distance + mouth_width) * 2)
        
        return {
            'expression_intensity': expression_intensity,
            'eye_openness': min(1.0, eye_distance * 10),
            'mouth_activity': min(1.0, mouth_width * 5),
            'alertness_indicator': expression_intensity > 0.3
        }
    
    async def _analyze_hand_gesture(self, landmarks) -> Dict[str, Any]:
        """Analyze hand gesture for consciousness indicators"""
        # Calculate hand openness based on finger spread
        thumb_tip = landmarks[4]
        index_tip = landmarks[8]
        middle_tip = landmarks[12]
        ring_tip = landmarks[16]
        pinky_tip = landmarks[20]
        
        # Calculate finger spread
        finger_spread = (
            abs(thumb_tip.x - index_tip.x) +
            abs(index_tip.x - middle_tip.x) +
            abs(middle_tip.x - ring_tip.x) +
            abs(ring_tip.x - pinky_tip.x)
        )
        
        gesture_confidence = min(1.0, finger_spread * 2)
        
        return {
            'gesture_confidence': gesture_confidence,
            'finger_spread': finger_spread,
            'hand_activity': gesture_confidence > 0.5
        }
    
    async def _analyze_body_posture(self, landmarks) -> Dict[str, Any]:
        """Analyze body posture for consciousness indicators"""
        # Key pose landmarks
        nose = landmarks[0]
        left_shoulder = landmarks[11]
        right_shoulder = landmarks[12]
        left_hip = landmarks[23]
        right_hip = landmarks[24]
        
        # Calculate posture alignment
        shoulder_alignment = abs(left_shoulder.y - right_shoulder.y)
        hip_alignment = abs(left_hip.y - right_hip.y)
        
        # Calculate overall alertness
        alertness_score = 1.0 - min(1.0, (shoulder_alignment + hip_alignment) * 2)
        
        return {
            'alertness_score': alertness_score,
            'posture_quality': alertness_score > 0.7,
            'shoulder_alignment': 1.0 - min(1.0, shoulder_alignment * 5),
            'confidence': 0.8
        }
    
    async def start_camera_processing(self, camera_index: int = 0) -> bool:
        """Start real-time camera processing"""
        try:
            cap = cv2.VideoCapture(camera_index)
            if not cap.isOpened():
                self.logger.error(f"Cannot open camera {camera_index}")
                return False
            
            self.processing_active = True
            self.logger.info(f"Started camera processing on camera {camera_index}")
            
            while self.processing_active:
                ret, frame = cap.read()
                if not ret:
                    self.logger.warning("Failed to read frame from camera")
                    break
                
                # Process frame
                consciousness_frame = await self.process_frame(frame)
                
                # Optional: Display frame with annotations
                if consciousness_frame.processed_data:
                    annotated_frame = await self._annotate_frame(frame, consciousness_frame)
                    cv2.imshow('MediaPipe Consciousness', annotated_frame)
                
                # Break on 'q' key
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
                
                # Maintain processing rate
                await asyncio.sleep(0.01)  # ~100 FPS max
            
            cap.release()
            cv2.destroyAllWindows()
            return True
            
        except Exception as e:
            self.logger.error(f"Error in camera processing: {e}")
            return False
    
    async def _annotate_frame(self, frame: np.ndarray, consciousness_frame: ConsciousnessFrame) -> np.ndarray:
        """Annotate frame with consciousness data"""
        annotated = frame.copy()
        
        # Add consciousness score
        cv2.putText(annotated, f"Consciousness: {consciousness_frame.consciousness_score:.3f}", 
                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Add confidence
        cv2.putText(annotated, f"Confidence: {consciousness_frame.confidence:.3f}", 
                   (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Add FPS
        cv2.putText(annotated, f"FPS: {self.fps_counter}", 
                   (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # Add processing mode
        cv2.putText(annotated, f"Mode: {self.current_mode.value}", 
                   (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return annotated
    
    async def get_consciousness_stats(self) -> Dict[str, Any]:
        """Get comprehensive consciousness processing statistics"""
        return {
            'processing_status': 'active' if self.processing_active else 'inactive',
            'current_mode': self.current_mode.value,
            'frame_count': self.frame_count,
            'fps': self.fps_counter,
            'history_size': len(self.consciousness_history),
            'average_processing_time_ms': np.mean(self.processing_times) * 1000 if self.processing_times else 0,
            'average_consciousness_score': np.mean(self.consciousness_scores) if self.consciousness_scores else 0,
            'latest_consciousness_score': self.consciousness_scores[-1] if self.consciousness_scores else 0,
            'performance_metrics': {
                'min_processing_time_ms': np.min(self.processing_times) * 1000 if self.processing_times else 0,
                'max_processing_time_ms': np.max(self.processing_times) * 1000 if self.processing_times else 0,
                'std_processing_time_ms': np.std(self.processing_times) * 1000 if self.processing_times else 0
            }
        }
    
    async def stop_processing(self):
        """Stop consciousness processing"""
        self.processing_active = False
        
        # Close processors
        if self.face_detection:
            self.face_detection.close()
        if self.face_mesh:
            self.face_mesh.close()
        if self.hands:
            self.hands.close()
        if self.pose:
            self.pose.close()
        if self.holistic:
            self.holistic.close()
        
        self.logger.info("MediaPipe consciousness processing stopped")

async def main():
    """Main demo of MediaPipe consciousness integration"""
    print("ðŸ“¹ GenAI OS - MediaPipe Consciousness Integration Demo")
    
    # Initialize processor
    processor = MediaPipeConsciousnessProcessor()
    
    # Initialize for holistic processing
    await processor.initialize_processors(ConsciousnessMode.HOLISTIC)
    
    print("ðŸš€ Starting camera processing... Press 'q' to quit")
    
    # Start camera processing
    success = await processor.start_camera_processing()
    
    if success:
        # Get final stats
        stats = await processor.get_consciousness_stats()
        print("\nðŸ“Š Final Processing Statistics:")
        print(f"  Total Frames: {stats['frame_count']}")
        print(f"  Final FPS: {stats['fps']}")
        print(f"  Average Processing Time: {stats['average_processing_time_ms']:.2f}ms")
        print(f"  Average Consciousness Score: {stats['average_consciousness_score']:.3f}")
        print(f"  History Size: {stats['history_size']}")
    
    # Stop processing
    await processor.stop_processing()
    print("âœ… MediaPipe consciousness integration demo complete!")

if __name__ == "__main__":
    asyncio.run(main())
