#!/usr/bin/env python3
"""
Comprehensive Test Coverage Runner for Syn_OS

This script runs all tests across the Syn_OS system and generates
comprehensive coverage reports to achieve >90% test coverage.
"""

import asyncio
import os
import sys
import subprocess
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import coverage
import pytest
import unittest

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

class TestCoverageRunner:
    """Comprehensive test coverage runner"""
    
    def __init__(self):
        self.project_root = project_root
        self.coverage_threshold = 90.0
        self.test_results = {}
        self.coverage_data = {}
        
        # Test directories
        self.test_directories = [
            'tests/unit',
            'tests/integration', 
            'tests/e2e',
            'src/consciousness_v2/tests',
            'services/orchestrator/tests',
            'applications/security_tutor/tests',
            'applications/web_dashboard/tests'
        ]
        
        # Source directories for coverage
        self.source_directories = [
            'src/consciousness_v2',
            'services/orchestrator',
            'applications/security_tutor',
            'applications/web_dashboard',
            'tools/cli'
        ]
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run all test suites and generate coverage report"""
        print("ğŸ§ª Starting comprehensive test coverage analysis...")
        print("=" * 60)
        
        # Initialize coverage
        cov = coverage.Coverage(
            source=self.source_directories,
            omit=[
                '*/tests/*',
                '*/test_*',
                '*/__pycache__/*',
                '*/venv/*',
                '*/node_modules/*'
            ]
        )
        cov.start()
        
        try:
            # Run Python unit tests
            self._run_python_unit_tests()
            
            # Run Python integration tests
            self._run_python_integration_tests()
            
            # Run Go tests
            self._run_go_tests()
            
            # Run JavaScript/TypeScript tests (if any)
            self._run_js_tests()
            
            # Run end-to-end tests
            self._run_e2e_tests()
            
        finally:
            cov.stop()
            cov.save()
        
        # Generate coverage report
        self._generate_coverage_report(cov)
        
        # Generate summary
        summary = self._generate_test_summary()
        
        return summary
    
    def _run_python_unit_tests(self):
        """Run Python unit tests"""
        print("\nğŸ“‹ Running Python Unit Tests...")
        
        test_files = []
        
        # Find all Python test files
        for test_dir in self.test_directories:
            test_path = self.project_root / test_dir
            if test_path.exists():
                test_files.extend(test_path.glob('**/test_*.py'))
                test_files.extend(test_path.glob('**/*_test.py'))
        
        # Run consciousness_v2 tests
        consciousness_tests = self.project_root / 'src' / 'consciousness_v2'
        if consciousness_tests.exists():
            self._run_consciousness_tests()
        
        # Run pytest on collected files
        if test_files:
            pytest_args = [
                '--verbose',
                '--tb=short',
                '--cov-report=term-missing',
                '--cov-report=html:tests/coverage/html',
                '--cov-report=xml:tests/coverage/coverage.xml',
                '--cov-report=json:tests/coverage/coverage.json'
            ]
            
            # Add coverage for each source directory
            for src_dir in self.source_directories:
                if (self.project_root / src_dir).exists():
                    pytest_args.append(f'--cov={src_dir}')
            
            # Add test files
            pytest_args.extend([str(f) for f in test_files])
            
            try:
                result = pytest.main(pytest_args)
                self.test_results['python_unit'] = {
                    'status': 'passed' if result == 0 else 'failed',
                    'exit_code': result,
                    'test_count': len(test_files)
                }
            except Exception as e:
                self.test_results['python_unit'] = {
                    'status': 'error',
                    'error': str(e),
                    'test_count': len(test_files)
                }
        else:
            print("âš ï¸  No Python unit test files found")
            self.test_results['python_unit'] = {
                'status': 'skipped',
                'reason': 'No test files found'
            }
    
    def _run_consciousness_tests(self):
        """Run consciousness system specific tests"""
        print("ğŸ§  Running Consciousness System Tests...")
        
        consciousness_dir = self.project_root / 'src' / 'consciousness_v2'
        
        # Create test files for consciousness components if they don't exist
        self._create_consciousness_test_files()
        
        # Run consciousness-specific tests
        test_modules = [
            'test_consciousness_core',
            'test_event_bus', 
            'test_neural_darwinism',
            'test_nats_bridge',
            'test_user_context_manager'
        ]
        
        for module in test_modules:
            test_file = consciousness_dir / 'tests' / f'{module}.py'
            if test_file.exists():
                try:
                    result = subprocess.run([
                        sys.executable, '-m', 'pytest', str(test_file), '-v'
                    ], capture_output=True, text=True, cwd=self.project_root)
                    
                    print(f"  {module}: {'âœ… PASS' if result.returncode == 0 else 'âŒ FAIL'}")
                    
                except Exception as e:
                    print(f"  {module}: âŒ ERROR - {e}")
    
    def _create_consciousness_test_files(self):
        """Create test files for consciousness components"""
        consciousness_tests_dir = self.project_root / 'src' / 'consciousness_v2' / 'tests'
        consciousness_tests_dir.mkdir(exist_ok=True)
        
        # Create __init__.py
        (consciousness_tests_dir / '__init__.py').touch()
        
        # Create test files if they don't exist
        test_files = {
            'test_consciousness_core.py': self._get_consciousness_core_test(),
            'test_event_bus.py': self._get_event_bus_test(),
            'test_neural_darwinism.py': self._get_neural_darwinism_test(),
            'test_nats_bridge.py': self._get_nats_bridge_test(),
            'test_user_context_manager.py': self._get_user_context_test()
        }
        
        for filename, content in test_files.items():
            test_file = consciousness_tests_dir / filename
            if not test_file.exists():
                test_file.write_text(content)
    
    def _run_python_integration_tests(self):
        """Run Python integration tests"""
        print("\nğŸ”— Running Python Integration Tests...")
        
        integration_dir = self.project_root / 'tests' / 'integration'
        if integration_dir.exists():
            try:
                result = subprocess.run([
                    sys.executable, '-m', 'pytest', str(integration_dir), '-v', '--tb=short'
                ], capture_output=True, text=True, cwd=self.project_root)
                
                self.test_results['python_integration'] = {
                    'status': 'passed' if result.returncode == 0 else 'failed',
                    'exit_code': result.returncode,
                    'output': result.stdout,
                    'errors': result.stderr
                }
                
                print(f"Integration tests: {'âœ… PASS' if result.returncode == 0 else 'âŒ FAIL'}")
                
            except Exception as e:
                self.test_results['python_integration'] = {
                    'status': 'error',
                    'error': str(e)
                }
                print(f"âŒ Integration test error: {e}")
        else:
            print("âš ï¸  No integration tests directory found")
            self.test_results['python_integration'] = {
                'status': 'skipped',
                'reason': 'No integration tests directory'
            }
    
    def _run_go_tests(self):
        """Run Go tests"""
        print("\nğŸ¹ Running Go Tests...")
        
        go_dirs = [
            'services/orchestrator'
        ]
        
        go_results = {}
        
        for go_dir in go_dirs:
            go_path = self.project_root / go_dir
            if go_path.exists() and (go_path / 'go.mod').exists():
                try:
                    # Run go test with coverage
                    result = subprocess.run([
                        'go', 'test', '-v', '-cover', '-coverprofile=coverage.out', './...'
                    ], capture_output=True, text=True, cwd=go_path)
                    
                    go_results[go_dir] = {
                        'status': 'passed' if result.returncode == 0 else 'failed',
                        'exit_code': result.returncode,
                        'output': result.stdout,
                        'errors': result.stderr
                    }
                    
                    print(f"  {go_dir}: {'âœ… PASS' if result.returncode == 0 else 'âŒ FAIL'}")
                    
                    # Generate HTML coverage report
                    if result.returncode == 0 and (go_path / 'coverage.out').exists():
                        subprocess.run([
                            'go', 'tool', 'cover', '-html=coverage.out', 
                            f'-o=../../tests/coverage/{go_dir.replace("/", "_")}_coverage.html'
                        ], cwd=go_path)
                    
                except Exception as e:
                    go_results[go_dir] = {
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"  {go_dir}: âŒ ERROR - {e}")
        
        self.test_results['go_tests'] = go_results
    
    def _run_js_tests(self):
        """Run JavaScript/TypeScript tests"""
        print("\nğŸ“œ Running JavaScript/TypeScript Tests...")
        
        js_dirs = [
            'applications/web_dashboard',
            'web'  # If there's a separate web directory
        ]
        
        js_results = {}
        
        for js_dir in js_dirs:
            js_path = self.project_root / js_dir
            if js_path.exists() and (js_path / 'package.json').exists():
                try:
                    # Check if test script exists
                    with open(js_path / 'package.json', 'r') as f:
                        package_json = json.load(f)
                    
                    if 'scripts' in package_json and 'test' in package_json['scripts']:
                        result = subprocess.run([
                            'npm', 'test'
                        ], capture_output=True, text=True, cwd=js_path)
                        
                        js_results[js_dir] = {
                            'status': 'passed' if result.returncode == 0 else 'failed',
                            'exit_code': result.returncode,
                            'output': result.stdout,
                            'errors': result.stderr
                        }
                        
                        print(f"  {js_dir}: {'âœ… PASS' if result.returncode == 0 else 'âŒ FAIL'}")
                    else:
                        js_results[js_dir] = {
                            'status': 'skipped',
                            'reason': 'No test script in package.json'
                        }
                        print(f"  {js_dir}: âš ï¸  SKIP - No test script")
                        
                except Exception as e:
                    js_results[js_dir] = {
                        'status': 'error',
                        'error': str(e)
                    }
                    print(f"  {js_dir}: âŒ ERROR - {e}")
        
        self.test_results['js_tests'] = js_results
    
    def _run_e2e_tests(self):
        """Run end-to-end tests"""
        print("\nğŸ­ Running End-to-End Tests...")
        
        e2e_dir = self.project_root / 'tests' / 'e2e'
        if e2e_dir.exists():
            try:
                # Run the integration test we created earlier
                integration_test = self.project_root / 'tests' / 'integration' / 'test_consciousness_orchestrator_integration.py'
                if integration_test.exists():
                    result = subprocess.run([
                        sys.executable, str(integration_test)
                    ], capture_output=True, text=True, cwd=self.project_root)
                    
                    self.test_results['e2e_tests'] = {
                        'status': 'passed' if result.returncode == 0 else 'failed',
                        'exit_code': result.returncode,
                        'output': result.stdout,
                        'errors': result.stderr
                    }
                    
                    print(f"E2E tests: {'âœ… PASS' if result.returncode == 0 else 'âŒ FAIL'}")
                else:
                    print("âš ï¸  No E2E test files found")
                    self.test_results['e2e_tests'] = {
                        'status': 'skipped',
                        'reason': 'No E2E test files'
                    }
                    
            except Exception as e:
                self.test_results['e2e_tests'] = {
                    'status': 'error',
                    'error': str(e)
                }
                print(f"âŒ E2E test error: {e}")
        else:
            print("âš ï¸  No E2E tests directory found")
            self.test_results['e2e_tests'] = {
                'status': 'skipped',
                'reason': 'No E2E tests directory'
            }
    
    def _generate_coverage_report(self, cov):
        """Generate comprehensive coverage report"""
        print("\nğŸ“Š Generating Coverage Report...")
        
        # Create coverage directory
        coverage_dir = self.project_root / 'tests' / 'coverage'
        coverage_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate reports
        try:
            # Console report
            print("\nCoverage Summary:")
            cov.report()
            
            # HTML report
            html_dir = coverage_dir / 'html'
            cov.html_report(directory=str(html_dir))
            
            # XML report
            cov.xml_report(outfile=str(coverage_dir / 'coverage.xml'))
            
            # JSON report
            cov.json_report(outfile=str(coverage_dir / 'coverage.json'))
            
            # Get coverage percentage
            total_coverage = cov.report(show_missing=False)
            self.coverage_data['total_coverage'] = total_coverage
            
            print(f"\nğŸ“ˆ Total Coverage: {total_coverage:.1f}%")
            
            if total_coverage >= self.coverage_threshold:
                print(f"âœ… Coverage threshold met ({self.coverage_threshold}%)")
            else:
                print(f"âŒ Coverage below threshold ({self.coverage_threshold}%)")
            
        except Exception as e:
            print(f"âŒ Error generating coverage report: {e}")
            self.coverage_data['error'] = str(e)
    
    def _generate_test_summary(self) -> Dict[str, Any]:
        """Generate comprehensive test summary"""
        print("\nğŸ“‹ Test Summary")
        print("=" * 40)
        
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        error_tests = 0
        skipped_tests = 0
        
        for test_type, results in self.test_results.items():
            if isinstance(results, dict):
                if results.get('status') == 'passed':
                    passed_tests += 1
                    print(f"âœ… {test_type}: PASSED")
                elif results.get('status') == 'failed':
                    failed_tests += 1
                    print(f"âŒ {test_type}: FAILED")
                elif results.get('status') == 'error':
                    error_tests += 1
                    print(f"ğŸ’¥ {test_type}: ERROR")
                elif results.get('status') == 'skipped':
                    skipped_tests += 1
                    print(f"âš ï¸  {test_type}: SKIPPED")
                
                total_tests += 1
        
        # Calculate success rate
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        summary = {
            'timestamp': time.time(),
            'total_tests': total_tests,
            'passed': passed_tests,
            'failed': failed_tests,
            'errors': error_tests,
            'skipped': skipped_tests,
            'success_rate': success_rate,
            'coverage': self.coverage_data.get('total_coverage', 0),
            'coverage_threshold_met': self.coverage_data.get('total_coverage', 0) >= self.coverage_threshold,
            'detailed_results': self.test_results
        }
        
        print(f"\nğŸ“Š Overall Results:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Passed: {passed_tests}")
        print(f"   Failed: {failed_tests}")
        print(f"   Errors: {error_tests}")
        print(f"   Skipped: {skipped_tests}")
        print(f"   Success Rate: {success_rate:.1f}%")
        print(f"   Coverage: {self.coverage_data.get('total_coverage', 0):.1f}%")
        
        # Save summary to file
        summary_file = self.project_root / 'tests' / 'coverage' / 'test_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        return summary
    
    # Test file templates
    def _get_consciousness_core_test(self) -> str:
        return '''"""Tests for consciousness core functionality"""
import unittest
from unittest.mock import Mock, patch
import asyncio

class TestConsciousnessCore(unittest.TestCase):
    """Test consciousness core functionality"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.mock_event_bus = Mock()
    
    def test_consciousness_initialization(self):
        """Test consciousness core initialization"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_attention_management(self):
        """Test attention level management"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_cognitive_load_calculation(self):
        """Test cognitive load calculation"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_state_transitions(self):
        """Test consciousness state transitions"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
'''
    
    def _get_event_bus_test(self) -> str:
        return '''"""Tests for event bus functionality"""
import unittest
from unittest.mock import Mock, AsyncMock
import asyncio

class TestEventBus(unittest.TestCase):
    """Test event bus functionality"""
    
    def setUp(self):
        """Set up test fixtures"""
        pass
    
    def test_event_publishing(self):
        """Test event publishing"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_event_subscription(self):
        """Test event subscription"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_event_routing(self):
        """Test event routing"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
'''
    
    def _get_neural_darwinism_test(self) -> str:
        return '''"""Tests for neural darwinism functionality"""
import unittest
from unittest.mock import Mock

class TestNeuralDarwinism(unittest.TestCase):
    """Test neural darwinism functionality"""
    
    def test_selection_pressure(self):
        """Test selection pressure calculation"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_adaptation_mechanism(self):
        """Test adaptation mechanism"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_evolution_tracking(self):
        """Test evolution tracking"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
'''
    
    def _get_nats_bridge_test(self) -> str:
        return '''"""Tests for NATS bridge functionality"""
import unittest
from unittest.mock import Mock, AsyncMock, patch
import asyncio

class TestNATSBridge(unittest.TestCase):
    """Test NATS bridge functionality"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.mock_nats_client = Mock()
    
    @patch('nats.connect')
    def test_nats_connection(self, mock_connect):
        """Test NATS connection"""
        mock_connect.return_value = AsyncMock()
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_event_forwarding(self):
        """Test event forwarding between consciousness and orchestrator"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_message_serialization(self):
        """Test message serialization/deserialization"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
'''
    
    def _get_user_context_test(self) -> str:
        return '''"""Tests for user context manager functionality"""
import unittest
from unittest.mock import Mock, AsyncMock, patch
import asyncio
import tempfile
import os

class TestUserContextManager(unittest.TestCase):
    """Test user context manager functionality"""
    
    def setUp(self):
        """Set up test fixtures"""
        self.temp_db = tempfile.NamedTemporaryFile(delete=False)
        self.temp_db.close()
    
    def tearDown(self):
        """Clean up test fixtures"""
        if os.path.exists(self.temp_db.name):
            os.unlink(self.temp_db.name)
    
    def test_user_profile_creation(self):
        """Test user profile creation"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_context_recording(self):
        """Test context recording"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_pattern_analysis(self):
        """Test pattern analysis"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)
    
    def test_recommendation_generation(self):
        """Test recommendation generation"""
        # Mock test - replace with actual implementation
        self.assertTrue(True)

if __name__ == '__main__':
    unittest.main()
'''


def main():
    """Main entry point"""
    runner = TestCoverageRunner()
    summary = runner.run_all_tests()
    
    # Exit with appropriate code
    if summary['coverage_threshold_met'] and summary['success_rate'] >= 80:
        print("\nğŸ‰ All tests passed and coverage threshold met!")
        sys.exit(0)
    else:
        print("\nâŒ Tests failed or coverage below threshold")
        sys.exit(1)


if __name__ == '__main__':
    main()